仅供参考

在智能代理（Agent）的开发中，管理上下文的方法已经从早期的“提示词工程”演进为系统化的**“上下文工程（Context Engineering）”**。其核心哲学是将大语言模型（LLM）视为系统的**CPU**，而将上下文窗口视为**RAM（内存）**，开发者负责管理哪些数据应当被加载进这块有限的“内存区”中进行推理。

以下是目前业界管理上下文的五大主流方法：

### 1. 上下文卸载（Context Offloading）
这种方法的核心理念是**不需要让所有信息都实时存在于 Agent 的消息队列中**，而是将非即时必要的数据转存到外部系统中。
*   **文件系统作为“终极上下文”**：Agent 学习按需读写文件（如 `todo.md` 或结构化笔记），将长期任务状态持久化到文件系统中。例如，**Claude Code** 和 **Codex** 利用 `AGENTS.md` 或 `CLAUDE.md` 文件记录项目规范和任务进度，使 Agent 具备持久的、跨会话的项目意识。
*   **外部状态存储**：将复杂的工具输出（如数千行的网页搜索结果）转存至沙盒环境或代理状态对象中，仅向模型返回一个极简的引用或路径，从而极大地节省 Token。

### 2. 缩减、剪枝与压缩（Reduction & Compression）
为了对抗**“上下文腐烂（Context Rot）”**（即性能随 Token 增加而下降），系统必须对输入进行精简。
*   **可逆压实（Compaction）**：Manus 采用的一种策略，即剥离那些可以从文件系统重建的信息（例如仅保留文件路径而非文件内容），这允许 Agent 在需要时随时恢复完整上下文。
*   **不可逆摘要（Summarization）**：当上下文接近上限时，使用结构化 **Schema（模式）** 对历史会话生成摘要。业界建议避免“自由文本摘要”，而应让模型填充预定义的表单（如：已修改的文件、当前目标、遗留问题），以防关键信息在压缩中丢失。
*   **上下文剪枝（Pruning）**：如 **Provence** 技术，利用轻量级模型识别并剔除检索到的文档中无关的句子，实现“零成本”上下文精炼。

### 3. 上下文隔离（Context Isolation）
通过架构设计防止不同任务之间的上下文产生“相互污染（Pollution）”。
*   **子代理架构（Subagents）**：这是最常见的手段。将复杂任务委派给拥有**独立上下文窗口**的专门子代理。子代理处理完细节后，仅将结果反馈给主代理，确保主进程的上下文始终保持“干净”且聚焦于战略目标。
*   **规划者-执行者解耦（Planner-Executor）**：如 **CoDA** 框架，规划者仅持有精简的战略指令和子任务结果，而将冗长的原始文档封锁在执行者的临时工作空间中。

### 4. 检索增强与分层记忆（Retrieval & Hierarchical Memory）
模拟人类认知，将记忆分为不同层级，仅在需要时“调入”内存。
*   **短期记忆（STM）与长期记忆（LTM）**：STM 负责当前会话的逻辑连贯，LTM 利用向量数据库或知识图谱存储历史偏好和海量知识。
*   **先进 RAG 策略**：
    *   **Contextual Retrieval**：在索引每个文档块之前，为其添加该文档的上下文背景（如所属文件名、章节主题），使检索准确率提升 49% 以上。
    *   **Graph-RAG**：检索实体间的关系网络而非孤立文本块，为模型提供更丰富的语义链接，支持多步复杂推理。

### 5. KV 缓存优化（KV Cache Optimization）
这是实现 Agent 高性能、低成本部署的最关键工程实践。
*   **前缀稳定性（Prefix Stability）**：保持系统提示词（System Prompt）不变。避免在开头放置动态的时间戳等标记，以确保能最大化复用已计算的缓存状态，从而降低 10 倍的成本。
*   **多轮隔离机制（FlowKV）**：为了防止在多轮对话中重复压缩早期历史导致的“灾难性遗忘”，FlowKV 仅对最新轮次的 KV 对进行隔离压缩，成功挽回了约 65% 的指令遵循性能。
*   **注意力操控（Recitation）**：在执行长任务时，Agent 会定期在上下文末尾**背诵（Recite）**其 Todo 列表，强行将全局目标拉回到模型的“近期注意力范围”内，防止其在长路径中迷失方向。

***
