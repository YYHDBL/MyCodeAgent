# 面试总结：MyCodeAgent 项目

> 面试场景：电话面试（口语化交流）
> 面试官角色：OpenCode 项目开发者，拥有丰富 Agent 开发经验
> 候选人项目：MyCodeAgent - 类 Claude Code 的 ReAct 智能编程助手
> 面试时间：2026-01-21

---

## 一、面试概述

### 1.1 项目背景

**MyCodeAgent** 是一个面向学习与实验的 ReAct 代码代理项目，核心技术栈包括：

- **ReAct 推理循环**（Thought → Action → Observation）
- **统一工具响应协议**（status/data/text/stats/context/error）
- **内置工具**：LS/Glob/Grep/Read/Write/Edit/MultiEdit/TodoWrite/Bash/Skill/Task
- **Skills 系统**：从 `skills/**/SKILL.md` 按需加载技能
- **Task 子代理**：general/explore/plan/summary 四种类型
- **上下文工程**：分层注入、历史压缩、工具输出截断
- **MCP 工具集成**：支持远程工具扩展

### 1.2 面试结构

| 轮次 | 问题数量 | 考察重点 | 难度梯度 |
|------|----------|----------|----------|
| 第一轮 | Q1-Q5 | 基础架构 → 压轴挑战 | ⭐ ~ ⭐⭐⭐⭐⭐ |
| 第二轮 | Q6-Q10 | 追问细节与边界处理 | ⭐⭐⭐ ~ ⭐⭐⭐⭐ |
| 第三轮 | Q11-Q15 | 实战经验与设计判断 | ⭐⭐⭐ ~ ⭐⭐⭐⭐⭐ |
| 第四轮 | Q16-Q20 | 系统观与未来思考 | ⭐⭐⭐⭐ ~ ⭐⭐⭐⭐⭐ |

---

## 二、第一轮：基础到压轴（Q1-Q5）

### Q1 (基础题) ReAct 循环实现

**问题：** 简历里你写了"基于 ReAct 范式实现自主代码库探索修改"。你实现的 ReAct 循环具体是怎么工作的？从用户输入到最终执行的流程是什么样的？你用的是什么方式来让 LLM 理解"现在应该思考、现在应该调用工具"？

**回答要点：**

- **实现方式：** "显式思考 + 显式工具调用"的 ReAct
- **流程：**
  1. 用户输入 → 输入预处理（清理、结构化）
  2. 拼接：系统提示词 + 工具提示词 + 历史上下文
  3. LLM 输出"思考/计划"，明确下一步是"直接回答"还是"调用工具"
  4. 工具调用采用严格的 function calling 结构
  5. 工具结果包装成统一结构，写回历史，LLM 继续下一轮
- **关键决策：** 没有靠"隐式推断"，而是让 LLM 明确判断何时思考、何时调用工具
- **收益：** 执行链路可追溯，工具调用失败能在"显式状态"中被处理

---

### Q2 (中等题) 工具注册机制

**问题：** 你提到设计了"工具注册机制，支持文件检索、编辑、Todo 等内置工具"。你是怎么设计工具接口的？每个工具的输入输出格式是什么样的？当工具执行失败或返回异常时，你是怎么让 Agent 感知并处理的？

**回答要点：**

- **统一响应协议设计：** 每个工具返回 `status/data/text/stats/context/error`
- **输入：** 统一用 JSON schema 描述参数（便于 LLM 生成结构化调用）
- **输出字段含义：**
  - `status`：成功/失败状态
  - `data`：结构化结果
  - `text`：人类可读摘要
  - `stats`：性能统计
  - `context`：执行上下文
  - `error`：错误信息
- **失败处理：**
  - 工具执行异常落在 `status=error` 和 `error` 字段
  - 统一的 trace logger 记录错误阶段和 traceback
  - 主 Agent 根据 status 决定重试、降级或转而总结
- **设计逻辑：** Agent 不用"猜测工具是否成功"，而是像读 API 响应一样读状态

---

### Q3 (中难题) 上下文压缩与输出截断

**问题：** 你提到"统一工具输出截断与结构化历史压缩机制"。具体是怎么做"截断"的？截断后怎么保证不会丢失关键信息？"结构化历史压缩"是用什么策略？

**回答要点：**

- **两层策略：**
  1. **工具输出截断：** 设定最大行数/字节数，支持 head/tail 或 head+tail，完整输出落盘到 `tool-output/`，截断摘要 + 文件引用写回历史
  2. **结构化历史压缩：** "分层总结"——保留最近 N 轮原文 + 历史摘要块（summary），带元数据（时间、来源、工具调用要点）
- **方案对比：**
  - A 方案：纯 token 截断 → 简单但信息丢失严重
  - B 方案：截断 + 外部落盘 + 摘要层 → 复杂但可追溯
- **选择：** B 方案，因为 Agent 的核心是"可追溯"；允许丢详细，但必须能回查

---

### Q4 (困难题) SubAgent 设计与隔离

**问题：** 当你把一个任务派发给 SubAgent 时，主 Agent 和 SubAgent 之间的上下文是怎么传递的？"权限隔离"是怎么实现的？有没有遇到过 SubAgent 的行为超出预期的情况？

**回答要点：**

- **SubAgent 类型：** general / explore / plan / summary
- **上下文传递：** 主 Agent "选择性注入"——给 SubAgent 缩短上下文（目标 + 关键事实 + 必要文件路径）
- **权限隔离（工具白名单）：**
  - `explore`：只读工具（list/read/search）
  - `plan`：只能输出计划，不可写
  - `summary`：只能压缩输出
  - `general`：可读写但会被主 Agent "审阅后执行"
- **危险操作限制：** 写/删必须走主 Agent 或写工具的安全检查
- **超预期情况：** SubAgent "过度自信"直接给最终方案但缺少验证
- **处理：** 主 Agent 在合并结果时做二次校验，加"可信度提示"

---

### Q5 (压轴题) 长链路稳定性保障

**问题：** 简历里你说"保障 Agent 在 200K 上下文窗口下稳定支持数十轮长链路任务"。在几十轮的对话中，你遇到过最棘手的问题是什么？你是怎么检测和处理这些情况的？

**回答要点：**

- **最棘手的问题：** 上下文漂移 + 工具输出爆炸
  - 几十轮后 Agent 忘掉早期目标或误用工具
  - 极长的工具输出导致模型关注点偏移
- **三件事处理：**
  1. **历史压缩 + 固定关键事实区：** 用 summary 机制保留目标与关键约束
  2. **工具输出截断 + 外部落盘：** 避免 token 被爆掉
  3. **轻量熔断：** 某工具连续失败就暂时禁用
- **检测机制：** 统计 token 用量、工具失败率、trace 日志中的异常模式
- **容错降级策略：**
  - 上下文接近阈值时自动压缩历史
  - 工具不可用时转为"只读探索 + 人工确认"

---

## 三、第二轮：追问细节（Q6-Q10）

### Q6 (追问 Q3) 压缩触发时机与摘要生成

**问题：** 压缩的触发时机是什么？摘要本身是怎么生成的？有没有出现过"摘要丢失了关键信息导致后面任务卡住"的情况？

**回答要点：**

- **触发时机：**
  - 估算 `estimated_total >= context_window * compression_threshold`（默认 10k * 0.8）
  - 且消息数 ≥ 3
  - 压缩时检查"轮次足够多"（默认 `min_retain_rounds=10`），避免刚积累一点就压
- **摘要生成：**
  - 用 LLM 生成，有超时控制（默认 120s）
  - 成本控制：摘要输入里工具输出只取前 500 字符
  - 超时或失败时降级为硬截断（保留最近 N 轮，不生成摘要）
- **风险与兜底：**
  - 承认"丢关键信息"的风险存在
  - 兜底一：保留最近 N 轮原文
  - 兜底二：关键事实由"Read 工具"重新读取，而不是依赖 Summary 记忆
- **核心认知：** 摘要不是"真相源"，只是让长链路稳定跑起来的工程折中

---

### Q7 (追问 Q2) MCP 工具集成细节

**问题：** MCP 工具的 schema 和你自己的工具 schema 格式不一样，你是怎么做适配的？有没有遇到过超时或服务不可用的情况？

**回答要点：**

- **注册流程：** 启动时从 MCP server `list_tools`，逐个注册成本地 Tool
- **适配方式：** MCP 的 `inputSchema` 映射成 `ToolParameter`；名字做清洗+去重
- **错误处理：**
  - 超时 → `MCP_TIMEOUT`
  - 连接失败 → `MCP_NETWORK_ERROR`
  - 解析/执行异常 → `MCP_PARSE_ERROR` / `MCP_EXECUTION_ERROR`
- **设计理念：** MCP 是"外挂增强"，失败也只是降级，不影响核心工具链

---

### Q8 (追问 Q4) SubAgent 协作与结果合并

**问题：** 当复杂任务需要多个 SubAgent 协作时，它们之间的结果是怎么传递的？如果两个 SubAgent 给出冲突的结论，怎么处理？

**回答要点：**

- **传递方式：** 主 Agent 中转，SubAgent 之间没有直接通信
- **冲突处理：** 没有自动仲裁算法，仍是主 Agent 评估
  - 再让 explore 补证据（文件路径、关键行号）
  - 或让 plan 基于证据重写计划
- **设计理念：** 宁可多走一步，也不让 SubAgent "自己拍板"，避免失控

---

### Q9 (追问 Q5) 跑偏检测与恢复

**问题：** 你是怎么检测 Agent 跑偏的？一旦发现跑偏了，恢复策略是什么？有没有遇到过"越纠越偏"的情况？

**回答要点：**

- **检测方式：** 没有全自动检测器，更多是轻量信号 + trace 观察
  - 连续空响应
  - 连续工具失败
  - 工具被拒（权限/参数）
  - 对目标复述变形（通过 summary/trace 可见）
- **恢复策略：**
  - 空响应：自动加"请给出最终答案或调用工具"的提示重试一次
  - 工具失败：把错误回传给模型，让模型修正
  - 必要时手动 `/save` / `/load` 回滚会话（不是自动）
- **经验总结：** 确实遇到过"越纠越偏"，所以现在更倾向"让模型先停下来确认目标或读文件"

---

### Q10 (综合) 性能与可观测性的权衡

**问题：** trace 记录本身也会消耗资源，你是怎么平衡可观测性和性能的？当 Agent 慢的时候，能通过 trace 快速定位瓶颈吗？

**回答要点：**

- **记录内容：**
  - system messages、context build、model_output、tool_call、tool_result、errors、compression 事件
  - token 使用统计和 tool 的 `time_ms`
- **存储方式：** JSONL + HTML 双轨（`memory/traces/`）
  - HTML：快速审计
  - JSONL：便于 grep/过滤
- **性能控制：**
  - `TRACE_ENABLED` 可直接关掉
  - `TRACE_SANITIZE` 默认开启，防泄露
  - HTML 里会对内容做截断
  - 原始 response 是否写入可开关
- **实战效果：**
  - 通常能从 tool 的 `time_ms` 和 summary 超时里迅速定位瓶颈
  - "trace 太多看不懂"发生过，解决：HTML 做成按 step 分块 + 截断

---

## 四、第三轮：实战经验（Q11-Q15）

### Q11 (Prompt 工程) 工具提示词的设计

**问题：** 工具描述是怎么写的？怎么平衡简洁性和准确性？Edit 和 MultiEdit 怎么区分？

**回答要点：**

- **设计风格：** "短而硬"
- **结构：** Purpose → Key Features → Parameters → Rules → Error codes → Examples
- **核心：** 规则与误用纠正（如 Edit 必须"先 Read、old_string 唯一、CONFLICT 就重读"）
- **Edit vs MultiEdit 区分：**
  - `Edit`：单点、唯一替换
  - `MultiEdit`：同一文件多个互不依赖的修改，一次原子提交
  - MultiEdit 强调"所有 old_string 都对原始文件匹配、不得重叠"
- **优化手段：**
  - "When to use"比长描述更有效
  - 把"常见错误"写成强提示
  - 有必要时用"自动注入参数"降低模型负担（mtime/size）

---

### Q12 (并发) SubAgent 能并行吗？

**问题：** 当任务可以拆成多个独立子任务时，你的系统是串行还是并行？如果不是，有没有考虑过增加并行能力？

**回答要点：**

- **当前状态：** 不并行，Task 工具同步跑 SubagentRunner
- **原因：**
  1. 工具执行和历史写入依赖顺序，强行并行会引入非确定性
  2. 多 SubAgent 并行时的合并策略复杂，容易结论"互相打架"
  3. 目标是稳定与可复现，所以先保证串行可控
- **未来可扩展思路：**
  - 对"纯读型探索"允许并行
  - 子任务结果由主 Agent 合并，并要求"带证据（路径/行号）"

---

### Q13 (测试) Agent 怎么测试？

**问题：** Agent 的行为是概率性的，你怎么做自动化测试？是测"最终结果"还是测"中间行为"？

**回答要点：**

- **测试策略：** 主要测试"确定性组件"，而不是模型输出
- **测试内容：**
  - 工具协议 & 工具行为：大量单测/协议合规测试
  - 上下文压缩、截断、Read/Edit 冲突检测等纯逻辑测试
  - Summary 这类 LLM 相关逻辑用 mock/patch
- **不做：** 不去断言"模型最终回答"是否一致，因为不稳定
- **线上保障：**
  - 严格工具协议
  - trace 日志
  - 错误回传
- **认知：** 用这三件事去定位和修复，而不是试图用测试去"锁死模型行为"

---

### Q14 (边界情况) 循环依赖与工具调用死锁

**问题：** 有没有可能形成某种形式的循环或死锁？遇到过类似的边界情况吗？

**回答要点：**

- **严格死锁：** 不会，因为工具是串行的，没有互相等待
- **可能情况：** 循环尝试（Read 截断 → 模型又试图用同一截断结果推理 → 发现不够 → 再 Read）
- **缓冲机制：**
  1. Read 的响应会告诉"还剩多少行，用 start_line 继续"
  2. 工具输出被截断时落盘到 `tool-output/`，返回路径
- **解决：** 把"继续读取"的路径明确告诉模型，减少盲目重试，加上最大步数限制

---

### Q15 (设计哲学) 什么时候不该用 Agent？

**问题：** 根据你的实战经验，哪些场景其实不适合用 Agent？有没有过"用 Agent 反而不如直接写代码快"的经历？

**回答要点：**

- **主动不用 Agent 的三个场景：**
  1. **确定性强、规则清晰的批量操作**（如大量文本替换、格式化）——脚本更稳更快
  2. **强一致性/合规场景**（如发布数据库迁移、财务变更）——Agent 不够可控
  3. **超低延迟场景**——实时请求链路里 Agent 不是最佳选择
- **核心判断：** Agent 更适合"探索性、半结构化、需要工具协作"的任务；"确定性流程、可脚本化的任务"直接写脚本
- **真实经历：** 确实有过"用 Agent 反而慢"的经历，现在的判断标准是"这事能不能 5 分钟脚本解决？能就不用 Agent"

---

## 五、第四轮：系统观与未来（Q16-Q20）

### Q16 (架构对比) 跟 Claude Code / OpenDevin 的差异

**问题：** 你觉得你的项目和 Claude Code（或 OpenDevin）最大的差异是什么？设计理念上的差异？

**回答要点：**

- **我的路线：** "小而可控、协议驱动、可追溯"
- **它们的路线：** "产品化的完整环境"，强调端到端能力和自动化
- **设计理念差异：** 更在意"每一步能解释、能回放"，而不是"最终自动把事做完"
- **权衡：** 宁愿牺牲一些自动化和并发，换来稳定和可复现

---

### Q17 (技术债务) 最想重构的点

**问题：** 哪些地方是你最想重构的？有没有"当时图省事写了个 if，现在想改架构"的情况？

**回答要点：**

- **三个想重构的点：**
  1. **工具提示词装载方式：** 现在写在 `prompts/tools_prompts/*.py` 里，直观但不够模块化；想改成结构化配置 + 自动裁剪
  2. **token 估算和上下文溢出处理：** 目前是字符数/3，粗糙；也没有"溢出后自动压缩重试"的机制
  3. **LLM 依赖层：** 调用封装比较薄，模型能力差异、错误分类、重试策略都还有改进空间

---

### Q18 (性能瓶颈) 最慢的是哪一块？

**问题：** 在实际使用中，最耗时的环节通常是哪个？如果只能优化一处，你会选哪个？

**回答要点：**

- **最大头：** LLM API 调用，其次是 Summary 生成（也是 LLM 调用）
- **工具侧：** 耗时主要来自大范围搜索（如 Grep），但整体还是 LLM 最慢
- **优化方向：**
  - 减少 token：缩短工具提示、压缩历史、按需注入信息
  - 降低 LLM 次数：把一些小操作合并、减少无必要的 summary 触发

---

### Q19 (错误恢复) 最难排查的 bug

**问题：** 你遇到过的最难排查的 bug 是什么？是怎么发现和解决的？

**回答要点：**

- **最难的问题：** "模型输出结构偶发异常"
  - 返回空 content 且没有 tool_calls，导致循环停住
  - 或 tool_call 缺 id，导致严格模式不通过
- **问题本质：** 不是传统逻辑 bug，而是"模型输出不稳定"的工程问题
- **解决方案：**
  - 加一次"空响应重试 + 明确提示"
  - 强制补 tool_call_id
  - 通过 trace 记录 raw response，确保能复盘
- **经验：** 这类 bug 很容易误判成"代码错了"，但其实是提示词和模型输出边界导致的

---

### Q20 (未来方向) 如果继续做会加什么

**问题：** 如果要继续往下做，你最想加的功能是什么？

**回答要点：**

- **还在维护：** 以实验和迭代为主
- **最想加的三块：**
  1. **更稳的上下文溢出恢复：** 检测 `context_length_exceeded` 后自动压缩+重试
  2. **只读并发能力：** 对纯读任务并行化，提高探索速度
  3. **更强的可观测性层：** 轻量的 trace viewer 或和 OpenTelemetry 对接
- **整体方向：** 在不牺牲可追溯性的前提下提升效率

---

## 六、面试官评估总结

### 6.1 亮点评价

| 亮点维度 | 具体表现 |
|----------|----------|
| **架构认知清晰** | 对项目定位明确："小而可控、协议驱动、可追溯"，知道和 Claude Code/OpenDevin 的差异 |
| **实战经验丰富** | 确实踩过坑：工具输出爆炸、上下文漂移、模型输出不稳定，能说出具体处理方式 |
| **边界意识强** | 知道 Agent 什么时候不该用，这个认识比"知道怎么用"更重要 |
| **工程思维扎实** | 决策基于"可控性"和"可追溯性"：SubAgent 不并行、测试只测确定性组件、用 trace 而不断言模型行为 |
| **诚实面对技术债务** | 能说出具体想重构的点，有自我反思能力 |

### 6.2 可以改进的地方

| 改进点 | 建议 |
|--------|------|
| **缺少具体数字** | 比如"LLM 调用占总时间的 70%"，说数字会更有说服力 |
| **MCP 部分可深入** | 可以准备 MCP schema mapping 的 edge case 案例 |
| **测试策略可更具体** | 准备 1-2 个具体测试用例，会让回答更生动 |

### 6.3 综合评分

| 维度 | 评分 | 说明 |
|------|------|------|
| 技术深度 | 8/10 | 对 ReAct、上下文工程、MCP 都有实际经验 |
| 系统设计 | 8/10 | 架构决策有思考，知道 trade-off |
| 工程能力 | 9/10 | trace、错误处理、技术债务认知都很到位 |
| 边界认知 | 9/10 | 知道 Agent 的局限，这个很难得 |
| 表达能力 | 8/10 | 逻辑清晰，但可以更多用数字说话 |

**总体评价：通过**

---

## 七、备考建议

### 7.1 故事类型优先级

如果准备一个"具体故事"，推荐**技术排障故事**：

- 为什么？能同时证明你会定位问题（trace/日志分析）+ 会解决问题（代码/策略调整）
- 面试官更容易把你代入："这个人来了之后，出问题他能修"
- 一个细节丰富的排障故事，本身就隐含了你对权衡的理解

**排障故事结构示例：**

> "有一次 Agent 处理一个长链路任务，在第 15 轮突然开始连续调用 Grep 而且每次结果都被截断。我看了 trace 发现：summary 里丢失了'目标文件'这个关键事实，模型一直在搜索。后来我把 summary 的 prompt 改成'必须保留所有文件路径'，问题解决了。"

这个故事包含了：现象 → 定位（trace）→ 根因（summary 丢失信息）→ 修复（prompt 调整）→ 结果。

### 7.2 数字类型优先级

| 优先级 | 数字类型 | 示例 | 为什么 |
|--------|----------|------|--------|
| 第一 | 成功率/失败率 | 工具调用成功率 95%、解析失败率 3%、每 50 轮约触发 1 次熔断 | 稳定性是 Agent 系统的核心诉求 |
| 第二 | 性能占比 | LLM 调用占 75%、工具执行占 20%、其他占 5% | 说明你了解系统瓶颈 |
| 第三 | 资源开销 | 平均每轮消耗 3K tokens、压缩后历史保留约 40% | 锦上添花，说明有成本意识 |

### 7.3 准备清单（优先级排序）

| 准备项 | 优先级 | 原因 |
|--------|--------|------|
| 技术排障故事 | ⭐⭐⭐ | 证明你能修问题 |
| 成功率/失败率数字 | ⭐⭐⭐ | 证明系统稳定 |
| 性能占比数字 | ⭐⭐ | 证明你懂瓶颈 |
| 产品化权衡故事 | ⭐⭐ | 证明你有思考 |
| 资源开销数字 | ⭐ | 锦上添花 |

---

## 八、完整问题索引

| 编号 | 问题主题 | 难度 | 核心考察点 |
|------|----------|------|------------|
| Q1 | ReAct 循环实现 | ⭐⭐ | 基础架构理解 |
| Q2 | 工具注册机制 | ⭐⭐ | 接口设计与错误处理 |
| Q3 | 上下文压缩与输出截断 | ⭐⭐⭐ | 上下文工程核心 |
| Q4 | SubAgent 设计与隔离 | ⭐⭐⭐⭐ | 权限控制与风险 |
| Q5 | 长链路稳定性保障 | ⭐⭐⭐⭐⭐ | 压轴综合能力 |
| Q6 | 压缩触发时机与摘要生成 | ⭐⭐⭐ | 细节决策 |
| Q7 | MCP 工具集成细节 | ⭐⭐⭐ | 外部系统集成 |
| Q8 | SubAgent 协作与结果合并 | ⭐⭐⭐⭐ | 复杂场景处理 |
| Q9 | 跑偏检测与恢复 | ⭐⭐⭐⭐ | 异常处理经验 |
| Q10 | 性能与可观测性权衡 | ⭐⭐⭐⭐ | 工程取舍 |
| Q11 | 工具提示词的设计 | ⭐⭐⭐ | Prompt 工程 |
| Q12 | SubAgent 并发能力 | ⭐⭐⭐ | 架构扩展性 |
| Q13 | Agent 测试策略 | ⭐⭐⭐⭐ | 质量保障思维 |
| Q14 | 循环依赖与死锁 | ⭐⭐⭐ | 边界情况处理 |
| Q15 | 什么时候不该用 Agent | ⭐⭐⭐⭐⭐ | 设计判断力 |
| Q16 | 与 Claude Code/OpenDevin 的差异 | ⭐⭐⭐ | 架构对比能力 |
| Q17 | 技术债务与重构 | ⭐⭐⭐ | 自我反思能力 |
| Q18 | 性能瓶颈分析 | ⭐⭐⭐ | 系统性能理解 |
| Q19 | 最难排查的 bug | ⭐⭐⭐⭐ | 问题解决能力 |
| Q20 | 未来发展方向 | ⭐⭐⭐ | 系统演进思考 |

---

## 九、核心知识点备忘

### 9.1 ReAct 循环

```
用户输入 → 预处理 → [系统提示 + 工具提示 + 历史上下文]
→ LLM 输出思考/计划 → 工具调用（function calling）
→ 工具结果（统一协议）→ 写回历史 → 下一轮
```

### 9.2 统一工具响应协议

```json
{
  "status": "success|error",
  "data": {},
  "text": "人类可读摘要",
  "stats": {"time_ms": 123},
  "context": {},
  "error": "错误信息"
}
```

### 9.3 SubAgent 类型与权限

| 类型 | 权限 | 用途 |
|------|------|------|
| explore | 只读 | 代码库探索 |
| plan | 只能输出计划 | 方案规划 |
| summary | 只能压缩输出 | 信息聚合 |
| general | 可读写（需审阅） | 通用执行 |

### 9.4 上下文压缩策略

- **触发条件：** `estimated_total >= context_window * compression_threshold` + `min_retain_rounds`
- **生成方式：** LLM 生成（超时降级为硬截断）
- **保留策略：** 最近 N 轮原文 + 历史摘要块

### 9.5 错误类型映射

| MCP 错误 | 映射结果 |
|----------|----------|
| 超时 | MCP_TIMEOUT |
| 连接失败 | MCP_NETWORK_ERROR |
| 解析异常 | MCP_PARSE_ERROR |
| 执行异常 | MCP_EXECUTION_ERROR |

---

*文档生成时间：2026-01-21*
*面试场景：模拟电话面试*
