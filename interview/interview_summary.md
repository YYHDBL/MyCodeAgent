# 面试复盘（逐题口语版 + 具体故事 + 数据模板）

> 目标：把所有不重复的问题记录成 **Qx / Ans** 口语化答案；每题稍微详细一点；补充“具体排障故事版本”和“带数据的回答模板”。

---

## 一、基础到中难：核心架构类

**Q1：ReAct 循环具体怎么跑？LLM 怎么知道啥时候思考、啥时候调工具？**
**Ans：**我走的是“显式工具调用”的 ReAct。流程是：
1) 先把 system prompt + 工具提示词 + history 拼成 messages；
2) 调 LLM，提取 `tool_calls`；
3) 有工具就执行，结果写入 history；
4) 下一轮继续。
LLM 之所以知道“该调工具”，是因为 system prompt 里明确了工具使用规则，并且给了结构化 schema。空响应时我会补一条提示再重试一次，避免模型“无话可说”。

**Q2：工具注册机制怎么设计？输入输出格式？出错怎么办？**
**Ans：**所有工具统一走一个 `Tool` 基类，输入参数有 schema 描述。输出统一成“标准信封”：
`status/data/text/stats/context/error`。
`status=error` 时配 `ErrorCode`，比如 `NOT_FOUND/INVALID_PARAM/CONFLICT`。工具失败不会让主流程崩掉，而是把错误回给模型，模型再修正下一步。

**Q3：工具输出太长怎么办？截断怎么做，怎么保证不丢关键信息？**
**Ans：**我做了统一的工具输出截断：超过行数/字节阈值就截断，并把完整输出落盘到 `tool-output/`。历史里只保留 preview 和 full_output_path。模型如果需要完整信息，就按路径继续 Read。这种“截断 + 落盘”的组合能保证上下文不被撑爆，同时不丢真正的原始数据。

**Q4：上下文压缩什么时候触发？摘要怎么生成？丢信息怎么办？**
**Ans：**触发条件是“估算 token >= 阈值”并且轮次够多。默认是 `context_window * 0.8`，且至少保留 10 轮。摘要用 LLM 生成，超时（默认 120s）就降级成硬截断。关键事实我不依赖摘要，模型需要时必须重新 Read 文件。

**Q5：SubAgent 怎么设计？上下文怎么传？权限怎么隔离？**
**Ans：**SubAgent 在同进程跑，但用独立 session（独立 messages）。权限只读：允许 `LS/Read/Grep/Glob/TodoWrite`，拒绝 `Write/Edit/Task/Bash`。主 Agent 负责合并结果和写入操作，子代理只产出分析与建议。

**Q6：SubAgent 没写权限，真要改文件怎么办？**
**Ans：**走 A：子代理输出“需要改什么 + 具体建议”，由父代理执行真正的 Write/Edit。这样保证写操作一定经过主 Agent 审核，避免子代理失控。

**Q7：SubAgent 能并行吗？**
**Ans：**当前不并行。因为并发会带来竞态和历史写入顺序问题，影响可追溯性。我更在意“稳定可控”。未来可以做“只读并发池”，但写操作依然串行。

**Q8：如果多个 SubAgent 结论冲突怎么办？**
**Ans：**主 Agent 做仲裁，通常会要求补证据（路径/行号），或者让 plan 重新生成基于证据的方案。不会让两个子代理直接争论，最终由主 Agent 拍板。

---

## 二、工具与容错类

**Q9：工具调用失败，比如 ls 不存在目录，会怎么处理？重试还是报错？**
**Ans：**直接返回 `status=error` + `ErrorCode=NOT_FOUND`。我不自动重试，模型看到错误后会修正路径或向用户确认。避免盲目重试浪费轮次。

**Q10：如果模型陷入死循环（反复调同一个失败工具）怎么办？**
**Ans：**有轻量熔断：同一工具连续失败达到阈值（默认 3 次）会临时禁用，返回 `CIRCUIT_OPEN`，同时把禁用列表写入提示词。再加最大步数限制，避免无穷循环。

**Q11：并发读写冲突怎么处理？有没有竞态？**
**Ans：**我没有做并发写入，核心靠乐观锁：Read 会记录 mtime/size，Edit/Write 必须带这个校验，不一致就返回 `CONFLICT`，强制 re-read。这样避免“读旧写新”的隐性冲突。

---

## 三、MCP 集成类

**Q12：MCP 工具怎么注册？schema 怎么适配？**
**Ans：**启动时 `list_tools`，对工具名做清洗+去重，然后把 MCP 的 inputSchema 转成本地 ToolParameter。调用时用适配器把 MCP 返回值转成统一协议信封。

**Q13：MCP 调用超时/服务挂了怎么办？**
**Ans：**统一返回 MCP_* 错误码（超时/网络/执行/解析），主流程继续跑，不影响核心工具链。MCP 是外挂，失败要可降级。

---

## 四、上下文与稳定性类

**Q14：200K 上下文下怎么稳定跑几十轮？会不会遗忘、跑偏？**
**Ans：**靠“历史压缩 + 工具输出截断 + 熔断 + 最大步数”。跑偏时主要靠 trace 定位根因，通常是摘要漏了关键事实或工具输出被截断误导。

**Q15：token 估算用字符数/3太粗糙，误差怎么办？**
**Ans：**这是预警阈值，不是精算。阈值设得保守（0.8），再配合压缩降级。未来可以替换 tiktoken，但当前是工程取舍。

**Q16：Summary 超时 120 秒怎么定的？有没有轻量方案？**
**Ans：**120 秒是折中：太短会失败，太长会拖慢。超时就直接降级硬截断（只保留最近 N 轮），这就是轻量方案。

---

## 五、Prompt 工程类

**Q17：工具提示词怎么写才不太长又够清楚？**
**Ans：**我用结构化模板：Purpose → Rules → Parameters → Error Codes → Examples。重点写“何时该用”和“常见误用”，比长解释更有效。

**Q18：Edit 和 MultiEdit 怎么让模型选对？**
**Ans：**提示词里直接写规则：Edit 只做“单点唯一替换”；MultiEdit 处理“同文件多处独立改动、原子提交”。强调“不要依赖中间状态、不要重叠编辑”。

---

## 六、测试与可观测性类

**Q19：Agent 怎么测试？模型行为不确定怎么办？**
**Ans：**我测确定性组件：工具协议、错误码、截断逻辑、乐观锁、熔断。LLM 输出不做严格断言，否则测试太脆弱。

**Q20：trace 是怎么串起来的？有标准格式吗？**
**Ans：**session_id 贯穿整个会话；每轮有 step；工具调用有 tool_call_id。trace 输出 JSONL + HTML，支持回放与审计。不是 OpenTelemetry，但预留扩展方向。

---

## 七、设计哲学与适用边界

**Q21：什么时候不该用 Agent？**
**Ans：**强一致性任务、可脚本化批量操作、低延迟实时场景。很多情况下脚本更快、更稳、更可控。

---

## 八、对比与技术债

**Q22：和 Claude Code / OpenDevin 的最大差异？**
**Ans：**我更强调“可控可追溯”，不追求全自动化。定位是实验场，而不是完整产品环境。

**Q23：最想重构的点？**
**Ans：**工具提示词组织方式、token 估算/溢出恢复机制、LLM 调用封装层。

**Q24：性能瓶颈在哪？**
**Ans：**主要是 LLM 调用，其次是 Summary 生成。工具执行（grep/read）通常不是最大头。

**Q25：最难排查的 bug 是啥？**
**Ans：**模型输出结构偶发异常，比如空 content 或缺 tool_call_id。修复靠 trace + 空响应重试 + tool_call_id 补齐。

**Q26：未来方向最想加什么？**
**Ans：**只读并发池、上下文溢出自动恢复、trace 可视化/标准化。

---

## 九、Skills 系统

**Q27：Skills 和工具的区别是什么？执行过程中如何交互？**
**Ans：**Skill 是提示词模板，不是工具。Skill 工具只负责加载/展开，然后进入正常 ReAct 循环，模型照样可以读文件、调用工具。

---

## 十、语言与技术选型

**Q28：为什么选 Python 而不是 TS？**
**Ans：**Python 迭代快、LLM 生态成熟，适合实验型项目；TS 类型安全、MCP 生态更好，适合产品化。目标偏实验，所以 Python 合理。

---

# 具体排障故事版本（可直接口述）

**故事 1：工具输出截断 + Summary 丢关键事实**
- 现象：一个长链路任务里，模型开始反复 Grep 同一个模块，每次结果都被截断，连续 5 轮没有进展。
- 定位：看 trace，发现 summary 里丢掉了“目标文件路径”这个关键事实，模型只能盲搜。
- 根因：summary prompt 没明确“必须保留文件路径/命名实体”，导致摘要丢关键信息。
- 修复：在 summary prompt 里加入“必须保留所有文件路径与关键符号名”，并在截断提示里加“如何继续 Read 的明确指引”。
- 结果：重复失败从 5 次降到 1 次以内，任务完成率明显提升。

**故事 2：模型输出结构异常导致流程卡住**
- 现象：某些模型偶发返回空 content 且无 tool_calls，主循环直接卡死。
- 定位：trace 里看到 raw response 是空 content，finish_reason 也异常。
- 修复：增加“空响应重试一次 + 明确提示必须给 content 或 tool_calls”，并强制补齐 tool_call_id。
- 结果：空响应引起的失败率明显下降，流程稳定性提升。

---

# 带数据的回答模板（面试时可替换具体数字）

> **模板 1：稳定性类**
- “我们把工具调用成功率做到 **XX%**，解析失败率压到 **YY%**，熔断触发频率大概 **每 ZZ 轮 1 次**。”

> **模板 2：性能占比类**
- “从 trace 看，**LLM 调用占总耗时约 XX%**，工具执行 **YY%**，上下文构建/压缩约 **ZZ%**。所以优化优先级是 LLM > Summary > 工具。”

> **模板 3：上下文成本类**
- “平均每轮消耗 **X.XK tokens**，压缩后历史保留约 **YY%**，工具输出截断后上下文体积下降 **ZZ%**。”

---

# 复盘要点（给自己看的）

- 优先准备“技术排障故事”，比泛泛谈更有说服力。
- 数字优先级：成功率/失败率 > 性能占比 > 资源开销。
- 简历表述：只写已实现；并发写“预留只读并发池架构”。
