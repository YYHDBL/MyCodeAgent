# ============================================
# MyCodeAgent 环境变量配置模板
# 复制此文件为 .env 并填入你的实际配置
# ============================================

# ===== LLM 全局配置 (OpenAI-compatible) =====
# LLM 供应商: openai | deepseek | zhipu | dashscope | modelscope | kimi | moonshot | siliconflow | ollama | vllm
LLM_PROVIDER=siliconflow

# LLM API 密钥
LLM_API_KEY=your_api_key_here

# LLM 服务地址
LLM_BASE_URL=https://api.siliconflow.cn/v1/chat/completions

# LLM 模型 ID
LLM_MODEL_ID=Pro/moonshotai/Kimi-K2.5

# LLM 超时时间（秒）
# LLM_TIMEOUT=120

# LLM 最大重试次数
# LLM_MAX_RETRIES=2


# ===== 轻量模型配置 (用于子代理/Task) =====
LIGHT_LLM_PROVIDER=deepseek
LIGHT_LLM_API_KEY=your_light_api_key_here
LIGHT_LLM_BASE_URL=https://api.deepseek.com
LIGHT_LLM_MODEL_ID=deepseek-chat


# ===== 上下文工程配置 =====
# 上下文窗口大小
CONTEXT_WINDOW=128000

# 压缩触发阈值 (0.0 - 1.0)
COMPRESSION_THRESHOLD=0.8

# 最少保留轮次数
MIN_RETAIN_ROUNDS=3


# ===== 工具输出截断配置 =====
# 截断方向: head | tail | head_tail
TOOL_OUTPUT_TRUNCATE_DIRECTION=head_tail

# 头尾保留行数
TOOL_OUTPUT_HEAD_TAIL_LINES=40


# ===== MCP 工具配置 =====
# Context7 MCP API Key
CTX7_API_KEY=your_ctx7_api_key_here

# Tavily Search API Key
TAVILY_API_KEY=your_tavily_api_key_here


# ===== GitHub 配置 (可选) =====
GITHUB_PAT_TOKEN=your_github_pat_here


# ===== 其他可选配置 =====
# 调试模式
# DEBUG=false

# 日志级别: DEBUG | INFO | WARNING | ERROR
# LOG_LEVEL=INFO

# 是否显示 ReAct 步骤
# SHOW_REACT_STEPS=true

# 温度参数 (0.0 - 2.0)
# TEMPERATURE=0.7

# 最大输出 token 数
# MAX_TOKENS=
